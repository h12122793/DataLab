{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T08:55:52.785344Z",
     "start_time": "2025-01-02T08:55:14.247457Z"
    }
   },
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from DataLab.purify.imputation import gain\n",
    "from DataLab.purify.imputation.gain import SGAIN, WSGAIN_CP, WSGAIN_GP\n",
    "from DataLab.GAIN.utils import rmse_loss\n",
    "import os\n",
    "\n",
    "########################################################################################################################\n",
    "# Research Centers:\n",
    "# -----------------\n",
    "# Centro ALGORITMI - School of Engineering – University of Minho\n",
    "# Braga - Portugal\n",
    "# http://algoritmi.uminho.pt/\n",
    "#\n",
    "# Medical Informatics Group\n",
    "# BIH - Berlin Institute of Health\n",
    "# Charité - Universitätsmedizin Berlin\n",
    "# https://www.bihealth.org/en/research/research-groups/fabian-prasser/\n",
    "#\n",
    "# Intelligent Analytics for Massive Data -- IAM\n",
    "# German Research Center for Artificial Intelligence -- DFKI\n",
    "# Deutsches Forschungszentrum für Künstliche Intelligenz -- DFKI\n",
    "# https://www.dfki.de/web/\n",
    "#\n",
    "#\n",
    "# Description:\n",
    "# ------------\n",
    "# This Python script is the program that allows to run the experiments described in [1].\n",
    "# One should be aware that exception handling to take care of incorrect data types, incorrect parameters' values, and\n",
    "# so forth is, typically, NOT performed, the rule is: We are all grown up (Python) programmers!\n",
    "#\n",
    "#\n",
    "# Moto:\n",
    "# -----\n",
    "# \"We think too much and feel too little. More than machinery we need humanity.\"\n",
    "#                         -- Excerpt of the final speech from The Great Dictator\n",
    "#\n",
    "#\n",
    "# Related Work:\n",
    "# -------------\n",
    "#   * https://github.com/epsilon-machine/missingpy\n",
    "#   * https://github.com/eltonlaw/impyute\n",
    "#   * https://github.com/iskandr/fancyimpute\n",
    "#   * https://github.com/kearnz/autoimpute\n",
    "#   * https://github.com/awslabs/datawig\n",
    "#   * https://scikit-learn.org/stable/modules/classes.html#module-sklearn.impute\n",
    "#   * https://www.statsmodels.org/stable/api.html?#imputation\n",
    "#   * https://github.com/jsyoon0823/GAIN\n",
    "#\n",
    "#\n",
    "# References:\n",
    "# -----------\n",
    "#  [1] Diogo Telmo Neves, Marcel Ganesh Naik, Alberto Proença,\n",
    "#      \"SGAIN, WSGAIN-CP and WSGAIN-GP: Novel GAN Methods for Missing Data Imputation,\"\n",
    "#      International Conference on Computational Science (ICCS), 2021.\n",
    "#  [2] Jinsung Yoon, James Jordon, Mihaela van der Schaar,\n",
    "#      \"GAIN: Missing Data Imputation using Generative Adversarial Nets,\"\n",
    "#      International Conference on Machine Learning (ICML), 2018.\n",
    "#  [3] Rubin, Donald B. \"Inference and missing data.\" Biometrika 63.3 (1976): 581-592.\n",
    "#  [4] Van Buuren, Stef. Flexible imputation of missing data. Chapman and Hall/CRC, 2018.\n",
    "#\n",
    "#\n",
    "# Authors:\n",
    "# --------\n",
    "# diogo telmo neves -- {dneves@di.uminho.de, diogo-telmo.neves@charite.de}\n",
    "#\n",
    "#\n",
    "# Copyright:\n",
    "# ----------\n",
    "# Copyright (c) 2020 diogo telmo neves.\n",
    "# All rights reserved.\n",
    "#\n",
    "#\n",
    "# Conditions:\n",
    "# -----------\n",
    "# This code is free/open source code but the following conditions must be met:\n",
    "#   * Redistributions of source code must retain the above copyright notice, this list of conditions and\n",
    "#     the following disclaimer in the documentation and/or other materials provided with the distribution.\n",
    "#   * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and\n",
    "#     the following disclaimer in the documentation and/or other materials provided with the distribution.\n",
    "#\n",
    "#\n",
    "# DISCLAIMER:\n",
    "# -----------\n",
    "# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES,\n",
    "# INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND\n",
    "# FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.\n",
    "# IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n",
    "# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n",
    "# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n",
    "# ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR\n",
    "# TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n",
    "# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
    "#\n",
    "#\n",
    "# Date:\n",
    "# -----\n",
    "# December 2020\n",
    "########################################################################################################################\n",
    "import sys\n",
    "\n",
    "from DataLab.GAIN.data_loader import data_loader\n",
    "\n",
    "\n",
    "from argparse import ArgumentParser, Namespace\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from time import time\n",
    "\n",
    "from typing import Any, Callable, Dict, List, Set, Tuple, Union\n",
    "\n",
    "########################################################################################################################\n",
    "# a few datasets from UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/index.php) and their metadata.\n",
    "# the metadata is useful for some basic data preprocessing tasks (e.g., label, ordinal, or\n",
    "# one-hot encoding of categorical variables, and min-max normalization of independent variables) as well as\n",
    "# for model instantiation and configuration.\n",
    "# the models are according to what was used in https://arxiv.org/abs/1806.02920 and in https://arxiv.org/abs/2006.11783\n",
    "########################################################################################################################\n",
    "DATASETS: Dict[str, Dict[str, Any]] = {\n",
    "    \"dataAllYear\": {\n",
    "        \"name\": \"data ALL YEARs\",\n",
    "        \"url\": \"\",\n",
    "        \"header\": [0],\n",
    "        \"drop_cols\": [\"admin1\", \"admin2\", \"psu\", \"hhid\", \"uhid\", \"hhweight\", \"country\"],  # columns to drop\n",
    "        \"categorical_vars\": {  # the cat. (i.e., discrete) vars. (i.e., features) that need to be encoded\n",
    "            \"urban\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"climatezone\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"head_male\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"head_literate\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"basiceduc\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"seceduc\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"fixedwater\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"fixedtoilet\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"quintile\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"singlefam\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"floor\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"wall\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"roof\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"health_insur\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"collectbiom\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"elec_grid\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"elec_any\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"elec_re\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"elec_dg\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"stove_biom\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"stove_krsn\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"stove_gas\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"stove_elec\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"waterheat_biom\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"waterheat_centr\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"waterheat_krsn\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"waterheat_gas\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"waterheat_elec\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"spaceheat_biom\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"spaceheat_centr\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"spaceheat_krsn\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"spaceheat_gas\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"spaceheat_elec\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"aircool_accool\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"aircool_fan\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"fridge\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"freezer\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"washmach\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"dishwash\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"dryer\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"iron\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"phone\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"vacclean\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"tv\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"music\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"pc\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"publictransport\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"car\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"scooter\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            },\n",
    "            \"bicycle\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            }\n",
    "        },\n",
    "        \"target\": \"fixedwater\",  # the  label of the dependent variable (i.e., feature)\n",
    "        \"scaler\": {\n",
    "            \"class\": MinMaxScaler,\n",
    "            \"feature_range\": (0, 1)\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"class\": LogisticRegression,\n",
    "            \"kwargs\": {\n",
    "                \"class_weight\": None,  # class_weight: dict or ‘balanced’, default=None\n",
    "                \"max_iter\": 3000,  # max_iter: int, default=100\n",
    "                \"n_jobs\": -1  # -1 means using all processors\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"2002_BRA\": {\n",
    "        \"name\": \"Breast Cancer Wisconsin (Diagnostic) Data Set\",\n",
    "        \"url\": \"https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)\",\n",
    "        \"header\": [0],\n",
    "        \"drop_cols\": [\"ID\"],  # columns to drop\n",
    "        \"categorical_vars\": {  # the cat. (i.e., discrete) vars. (i.e., features) that need to be encoded\n",
    "            \"urban\": {\n",
    "                \"class\": OneHotEncoder,\n",
    "                \"kwargs\": {\"dtype\": int}\n",
    "            }, \"hhsize\": {\n",
    "                \"class\": OneHotEncoder,\n",
    "                \"kwargs\": {\"dtype\": int}\n",
    "            }, \"head_literate\": {\n",
    "                \"class\": OneHotEncoder,\n",
    "                \"kwargs\": {\"dtype\": int}\n",
    "            }, \"tv\": {\n",
    "                \"class\": OneHotEncoder,\n",
    "                \"kwargs\": {\"dtype\": int}\n",
    "            }, \"music\": {\n",
    "                \"class\": OneHotEncoder,\n",
    "                \"kwargs\": {\"dtype\": int}\n",
    "            }, \"fridge\": {\n",
    "                \"class\": OneHotEncoder,\n",
    "                \"kwargs\": {\"dtype\": int}\n",
    "            },\n",
    "        },\n",
    "        \"target\": \"exp\",  # the  label of the dependent variable (i.e., feature)\n",
    "        \"scaler\": {\n",
    "            \"class\": MinMaxScaler,\n",
    "            \"feature_range\": (-1, 1)\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"class\": HistGradientBoostingRegressor,\n",
    "            \"kwargs\": {\n",
    "\n",
    "\n",
    "                \"max_iter\": 3000,  # max_iter: int, default=100\n",
    "\n",
    "\n",
    "                \"max_depth\": 5,\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"breast\": {\n",
    "        \"name\": \"Breast Cancer Wisconsin (Diagnostic) Data Set\",\n",
    "        \"url\": \"https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)\",\n",
    "        \"header\": [0],\n",
    "        \"drop_cols\": [\"ID\"],  # columns to drop\n",
    "        \"categorical_vars\": {  # the cat. (i.e., discrete) vars. (i.e., features) that need to be encoded\n",
    "            \"Diagnosis\": {\n",
    "                \"class\": LabelEncoder,\n",
    "                \"kwargs\": {},\n",
    "            }\n",
    "        },\n",
    "        \"target\": \"Diagnosis\",  # the  label of the dependent variable (i.e., feature)\n",
    "        \"scaler\": {\n",
    "            \"class\": MinMaxScaler,\n",
    "            \"feature_range\": (-1, 1)\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"class\": LogisticRegression,\n",
    "            \"kwargs\": {\n",
    "                \"class_weight\": None,  # class_weight: dict or ‘balanced’, default=None\n",
    "                \"max_iter\": 3000,  # max_iter: int, default=100\n",
    "                \"n_jobs\": -1  # -1 means using all processors\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"covertype\": {\n",
    "        \"name\": \"Covertype Data Set\",\n",
    "        \"url\": \"https://archive.ics.uci.edu/ml/datasets/Covertype\",\n",
    "        \"header\": [0],\n",
    "        \"drop_cols\": [],  # columns to drop\n",
    "        \"categorical_vars\": None,  # in this case, for the sake of simplicity we do NOT enumerate the cat. vars.\n",
    "        \"target\": \"Cover_Type\",  # the  label of the dependent variable (i.e., feature)\n",
    "        \"scaler\": {\n",
    "            \"class\": MinMaxScaler,\n",
    "            \"feature_range\": (-1, 1)\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"class\": LogisticRegression,\n",
    "            \"kwargs\": {\n",
    "                \"class_weight\": None,  # class_weight: dict or ‘balanced’, default=None\n",
    "                \"max_iter\": 1000,  # max_iter: int, default=100\n",
    "                \"n_jobs\": -1  # -1 means using all processors\n",
    "            }\n",
    "        }\n",
    "        # \"model\": {\n",
    "        #     \"class\": KNeighborsClassifier,\n",
    "        #     \"kwargs\": {\n",
    "        #         \"n_neighbors\": 5,\n",
    "        #         \"weights\": 'uniform',    # weights: {‘uniform’, ‘distance’} or callable, default=’uniform’\n",
    "        #         \"algorithm\": 'auto',     # algorithm: {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, default=’auto’\n",
    "        #         \"p\": 2,                  # p: int, default=2 (p = 1 --> l1, manhattan; p = 2 --> l2, euclidean)\n",
    "        #         \"n_jobs\": -1             # -1 means using all processors\n",
    "        #     }\n",
    "        # }\n",
    "    },\n",
    "    \"credit\": {  # TODO: NEWS MORE ITERATIONS FOR THE MODEL TO CONVERGE\n",
    "        \"name\": \"Default of Credit Card Clients Data Set\",\n",
    "        \"url\": \"https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients\",\n",
    "        \"header\": [0],\n",
    "        \"drop_cols\": [\"ID\"],  # columns to drop\n",
    "        \"categorical_vars\": {  # the cat. (i.e., discrete) vars. (i.e., features) that need to be encoded\n",
    "            \"SEX\": {\n",
    "                \"class\": OneHotEncoder,\n",
    "                \"kwargs\": {\"dtype\": int}\n",
    "            },\n",
    "            \"EDUCATION\": {\n",
    "                \"class\": OneHotEncoder,\n",
    "                \"kwargs\": {\"dtype\": int}\n",
    "            },\n",
    "            \"MARRIAGE\": {\n",
    "                \"class\": OneHotEncoder,\n",
    "                \"kwargs\": {\"dtype\": int}\n",
    "            },\n",
    "            \"PAY_1\": {\n",
    "                \"class\": OneHotEncoder,\n",
    "                \"kwargs\": {\"dtype\": int}\n",
    "            },\n",
    "            \"PAY_2\": {\n",
    "                \"class\": OneHotEncoder,\n",
    "                \"kwargs\": {\"dtype\": int}\n",
    "            },\n",
    "            \"PAY_3\": {\n",
    "                \"class\": OneHotEncoder,\n",
    "                \"kwargs\": {\"dtype\": int}\n",
    "            },\n",
    "            \"PAY_4\": {\n",
    "                \"class\": OneHotEncoder,\n",
    "                \"kwargs\": {\"dtype\": int}\n",
    "            },\n",
    "            \"PAY_5\": {\n",
    "                \"class\": OneHotEncoder,\n",
    "                \"kwargs\": {\"dtype\": int}\n",
    "            },\n",
    "            \"PAY_6\": {\n",
    "                \"class\": OneHotEncoder,\n",
    "                \"kwargs\": {\"dtype\": int}\n",
    "            }\n",
    "        },\n",
    "        \"target\": \"def. pay. n. m.\",  # the  label of the dependent variable (i.e., feature)\n",
    "        \"scaler\": {\n",
    "            \"class\": MinMaxScaler,\n",
    "            \"feature_range\": (-1, 1)\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"class\": LogisticRegression,\n",
    "            \"kwargs\": {\n",
    "                \"class_weight\": None,  # class_weight: dict or ‘balanced’, default=None\n",
    "                \"max_iter\": 1200,  # max_iter: int, default=100\n",
    "                \"n_jobs\": -1  # -1 means using all processors\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"eeg\": {\n",
    "        \"name\": \"EEG Eye State Data Set\",\n",
    "        \"url\": \"https://archive.ics.uci.edu/ml/datasets/EEG+Eye+State\",\n",
    "        \"header\": [0],\n",
    "        \"drop_cols\": [],  # columns to drop\n",
    "        \"categorical_vars\": None,  # None --> NO categorical (i.e., discrete) variables (i.e., features)\n",
    "        \"target\": \"Eye Detection\",  # the label of the dependent variable (i.e., feature)\n",
    "        \"scaler\": {\n",
    "            \"class\": MinMaxScaler,\n",
    "            \"feature_range\": (-1, 1)\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"class\": KNeighborsClassifier,\n",
    "            \"kwargs\": {\n",
    "                \"n_neighbors\": 5,\n",
    "                \"weights\": 'uniform',  # weights: {‘uniform’, ‘distance’} or callable, default=’uniform’\n",
    "                \"algorithm\": 'auto',  # algorithm: {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, default=’auto’\n",
    "                \"p\": 2,  # p: int, default=2 (p = 1 --> l1, manhattan; p = 2 --> l2, euclidean)\n",
    "                \"n_jobs\": -1  # -1 means using all processors\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"iris\": {\n",
    "        \"name\": \"Iris Data Set\",\n",
    "        \"url\": \"https://archive.ics.uci.edu/ml/datasets/iris\",\n",
    "        \"header\": [0],\n",
    "        \"drop_cols\": [],  # columns to drop\n",
    "        \"categorical_vars\": None,  # None --> NO categorical (i.e., discrete) variables (i.e., features)\n",
    "        \"target\": \"class\",  # the label of the dependent variable (i.e., feature)\n",
    "        \"scaler\": {\n",
    "            \"class\": MinMaxScaler,\n",
    "            \"feature_range\": (-1, 1)\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"class\": KNeighborsClassifier,\n",
    "            \"kwargs\": {\n",
    "                \"n_neighbors\": 5,\n",
    "                \"weights\": 'uniform',  # weights: {‘uniform’, ‘distance’} or callable, default=’uniform’\n",
    "                \"algorithm\": 'auto',  # algorithm: {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, default=’auto’\n",
    "                \"p\": 2,  # p: int, default=2 (p = 1 --> l1, manhattan; p = 2 --> l2, euclidean)\n",
    "                \"n_jobs\": -1  # -1 means using all processors\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"letter\": {\n",
    "        \"name\": \"Letter Recognition Data Set\",\n",
    "        \"url\": \"https://archive.ics.uci.edu/ml/datasets/letter+recognition\",\n",
    "        \"header\": [0],\n",
    "        \"drop_cols\": [],  # columns to drop\n",
    "        \"categorical_vars\": None,  # None --> NO categorical (i.e., discrete) variables (i.e., features)\n",
    "        \"target\": \"letter\",  # the label of the dependent variable (i.e., feature)\n",
    "        \"scaler\": {\n",
    "            \"class\": MinMaxScaler,\n",
    "            \"feature_range\": (-1, 1)\n",
    "        },\n",
    "        # \"model\": {\n",
    "        #     \"class\": LogisticRegression,\n",
    "        #     \"kwargs\": {\n",
    "        #         \"class_weight\": None,    # class_weight: dict or ‘balanced’, default=None\n",
    "        #         \"max_iter\": 300,         # max_iter: int, default=100\n",
    "        #         \"n_jobs\": -1             # -1 means using all processors\n",
    "        #     }\n",
    "        # }\n",
    "        # \"model\": {\n",
    "        #     \"class\": SVC,\n",
    "        #     \"kwargs\": {\n",
    "        #         \"C\": 1000,               # C: float, default=1.0\n",
    "        #         \"kernel\": \"rbf\",         # kernel: {‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’}, default=’rbf’\n",
    "        #         \"gamma\": 0.01,           # gamma: {‘scale’, ‘auto’} or float, default=’scale’\n",
    "        #         \"probability\": True      # probability: bool, default=False\n",
    "        #     }\n",
    "        # }\n",
    "        \"model\": {\n",
    "            \"class\": KNeighborsClassifier,\n",
    "            \"kwargs\": {\n",
    "                \"n_neighbors\": 10,\n",
    "                \"weights\": 'uniform',  # weights: {‘uniform’, ‘distance’} or callable, default=’uniform’\n",
    "                \"algorithm\": 'ball_tree',  # algorithm: {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, default=’auto’\n",
    "                \"p\": 2,  # p: int, default=2 (p = 1 --> l1, manhattan; p = 2 --> l2, euclidean)\n",
    "                \"n_jobs\": -1  # -1 means using all processors\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mushroom\": {\n",
    "        \"name\": \"Mushroom Data Set\",\n",
    "        \"url\": \"https://archive.ics.uci.edu/ml/datasets/Mushroom\",\n",
    "        \"header\": [0],\n",
    "        \"drop_cols\": [\"stalk-root\"],  # columns to drop\n",
    "        \"categorical_vars\": {  # the cat. (i.e., discrete) vars. (i.e., features) that need to be encoded\n",
    "            \"cap-shape\": {\n",
    "                \"class\": OneHotEncoder,\n",
    "                \"kwargs\": {\"dtype\": int}\n",
    "            },\n",
    "            \"cap-surface\": {\n",
    "                \"class\": OneHotEncoder,\n",
    "                \"kwargs\": {\"dtype\": int}\n",
    "            },\n",
    "            \"cap-color\": {\n",
    "                \"class\": OneHotEncoder,\n",
    "                \"kwargs\": {\"dtype\": int}\n",
    "            },\n",
    "            \"bruises\": {\n",
    "                \"class\": OneHotEncoder,\n",
    "                \"kwargs\": {\"dtype\": int}\n",
    "            },\n",
    "            \"odor\": {\n",
    "                \"class\": OneHotEncoder,\n",
    "                \"kwargs\": {\"dtype\": int}\n",
    "            },\n",
    "            \"gill-attachment\": {\n",
    "                \"class\": OneHotEncoder,\n",
    "                \"kwargs\": {\"dtype\": int}\n",
    "            },\n",
    "            \"gill-spacing\": {\n",
    "                \"class\": OneHotEncoder,\n",
    "                \"kwargs\": {\"dtype\": int}\n",
    "            },\n",
    "            \"gill-size\": {\n",
    "                \"class\": OneHotEncoder,\n",
    "                \"kwargs\": {\"dtype\": int}\n",
    "            },\n",
    "            \"gill-color\": {\n",
    "                \"class\": OneHotEncoder,\n",
    "                \"kwargs\": {\"dtype\": int}\n",
    "            },\n",
    "            \"stalk-shape\": {\n",
    "                \"class\": OneHotEncoder,\n",
    "                \"kwargs\": {\"dtype\": int}\n",
    "            },\n",
    "            \"stalk-root\": {\n",
    "                \"class\": OneHotEncoder,\n",
    "                \"kwargs\": {\"dtype\": int}\n",
    "            },\n",
    "            \"stalk-surface-above-ring\": {\n",
    "                \"class\": OneHotEncoder,\n",
    "                \"kwargs\": {\"dtype\": int}\n",
    "            },\n",
    "            \"stalk-surface-below-ring\": {\n",
    "                \"class\": OneHotEncoder,\n",
    "                \"kwargs\": {\"dtype\": int}\n",
    "            },\n",
    "            \"stalk-color-above-ring\": {\n",
    "                \"class\": OneHotEncoder,\n",
    "                \"kwargs\": {\"dtype\": int}\n",
    "            },\n",
    "            \"stalk-color-below-ring\": {\n",
    "                \"class\": OneHotEncoder,\n",
    "                \"kwargs\": {\"dtype\": int}\n",
    "            },\n",
    "            \"veil-type\": {\n",
    "                \"class\": OneHotEncoder,\n",
    "                \"kwargs\": {\"dtype\": int}\n",
    "            },\n",
    "            \"veil-color\": {\n",
    "                \"class\": OneHotEncoder,\n",
    "                \"kwargs\": {\"dtype\": int}\n",
    "            },\n",
    "            \"ring-number\": {\n",
    "                \"class\": OneHotEncoder,\n",
    "                \"kwargs\": {\"dtype\": int}\n",
    "            },\n",
    "            \"ring-type\": {\n",
    "                \"class\": OneHotEncoder,\n",
    "                \"kwargs\": {\"dtype\": int}\n",
    "            },\n",
    "            \"spore-print-color\": {\n",
    "                \"class\": OneHotEncoder,\n",
    "                \"kwargs\": {\"dtype\": int}\n",
    "            },\n",
    "            \"population\": {\n",
    "                \"class\": OneHotEncoder,\n",
    "                \"kwargs\": {\"dtype\": int}\n",
    "            },\n",
    "            \"habitat\": {\n",
    "                \"class\": OneHotEncoder,\n",
    "                \"kwargs\": {\"dtype\": int}\n",
    "            }\n",
    "        },\n",
    "        \"target\": \"class\",  # the label of the dependent variable (i.e., feature)\n",
    "        \"scaler\": {\n",
    "            \"class\": MinMaxScaler,\n",
    "            \"feature_range\": (-1, 1)\n",
    "        },\n",
    "        # \"model\": {\n",
    "        #     \"class\": LogisticRegression,\n",
    "        #     \"kwargs\": {\n",
    "        #         \"class_weight\": None,    # class_weight: dict or ‘balanced’, default=None\n",
    "        #         \"max_iter\": 1000,        # max_iter: int, default=100\n",
    "        #         \"n_jobs\": -1             # -1 means using all processors\n",
    "        #     }\n",
    "        # }\n",
    "        \"model\": {\n",
    "            \"class\": KNeighborsClassifier,\n",
    "            \"kwargs\": {\n",
    "                \"n_neighbors\": 5,\n",
    "                \"weights\": 'uniform',  # weights: {‘uniform’, ‘distance’} or callable, default=’uniform’\n",
    "                \"algorithm\": 'auto',  # algorithm: {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, default=’auto’\n",
    "                \"p\": 2,  # p: int, default=2 (p = 1 --> l1, manhattan; p = 2 --> l2, euclidean)\n",
    "                \"n_jobs\": -1  # -1 means using all processors\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"news\": {  # PORTUGUESE --> ALGORITMI Research Centre, UMinho, Portugal\n",
    "        \"name\": \"Online News Popularity Data Set\",\n",
    "        \"url\": \"https://archive.ics.uci.edu/ml/datasets/online+news+popularity\",\n",
    "        \"header\": [0],\n",
    "        \"drop_cols\": [\"url\"],  # columns to drop\n",
    "        \"categorical_vars\": None,  # None --> NO categorical (i.e., discrete) variables (i.e., features)\n",
    "        \"target\": \"shares\",  # the label of the dependent variable (i.e., feature)\n",
    "        \"scaler\": {\n",
    "            \"class\": MinMaxScaler,\n",
    "            \"feature_range\": (-1, 1)\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"class\": LogisticRegression,\n",
    "            \"kwargs\": {\n",
    "                \"class_weight\": None,  # class_weight: dict or ‘balanced’, default=None\n",
    "                \"max_iter\": 1000,  # max_iter: int, default=100\n",
    "                \"n_jobs\": -1  # -1 means using all processors\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"spam\": {\n",
    "        \"name\": \"Spambase Data Set\",\n",
    "        \"url\": \"https://archive.ics.uci.edu/ml/datasets/spambase\",\n",
    "        \"header\": [0],\n",
    "        \"drop_cols\": [],  # columns to drop\n",
    "        \"categorical_vars\": None,  # None --> NO categorical (i.e., discrete) variables (i.e., features)\n",
    "        \"target\": \"spam\",  # the label of the dependent variable (i.e., feature)\n",
    "        \"scaler\": {\n",
    "            \"class\": MinMaxScaler,\n",
    "            \"feature_range\": (-1, 1)\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"class\": LogisticRegression,\n",
    "            \"kwargs\": {\n",
    "                \"class_weight\": None,  # class_weight: dict or ‘balanced’, default=None\n",
    "                \"max_iter\": 2000,  # max_iter: int, default=100\n",
    "                \"n_jobs\": -1  # -1 means using all processors\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"wine-red\": {  # PORTUGUESE --> DSI, UMinho, Portugal\n",
    "        \"name\": \"Yeast Data Set\",\n",
    "        \"url\": \"https://archive.ics.uci.edu/ml/datasets/wine+quality\",\n",
    "        \"header\": [0],\n",
    "        \"drop_cols\": [],  # columns to drop\n",
    "        \"categorical_vars\": None,  # None --> NO categorical (i.e., discrete) variables (i.e., features)\n",
    "        \"target\": \"quality\",  # the label of the dependent variable (i.e., feature)\n",
    "        \"scaler\": {\n",
    "            \"class\": MinMaxScaler,\n",
    "            \"feature_range\": (0, 1)\n",
    "        },\n",
    "        # \"model\": {\n",
    "        #     \"class\": LogisticRegression,\n",
    "        #     \"kwargs\": {\n",
    "        #         \"class_weight\": None,    # class_weight: dict or ‘balanced’, default=None\n",
    "        #         \"max_iter\": 1000,        # max_iter: int, default=100\n",
    "        #         \"n_jobs\": -1             # -1 means using all processors\n",
    "        #     }\n",
    "        # }\n",
    "        \"model\": {\n",
    "            \"class\": KNeighborsClassifier,\n",
    "            \"kwargs\": {\n",
    "                \"n_neighbors\": 5,\n",
    "                \"weights\": 'uniform',  # weights: {‘uniform’, ‘distance’} or callable, default=’uniform’\n",
    "                \"algorithm\": 'auto',  # algorithm: {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, default=’auto’\n",
    "                \"p\": 2,  # p: int, default=2 (p = 1 --> l1, manhattan; p = 2 --> l2, euclidean)\n",
    "                \"n_jobs\": -1  # -1 means using all processors\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"wine-white\": {  # PORTUGUESE --> DSI, UMinho, Portugal\n",
    "        \"name\": \"Yeast Data Set\",\n",
    "        \"url\": \"https://archive.ics.uci.edu/ml/datasets/wine+quality\",\n",
    "        \"header\": [0],\n",
    "        \"drop_cols\": [],  # columns to drop\n",
    "        \"categorical_vars\": None,  # None --> NO categorical (i.e., discrete) variables (i.e., features)\n",
    "        \"target\": \"quality\",  # the label of the dependent variable (i.e., feature)\n",
    "        \"scaler\": {\n",
    "            \"class\": MinMaxScaler,\n",
    "            \"feature_range\": (0, 1)\n",
    "        },\n",
    "        # \"model\": {\n",
    "        #     \"class\": LogisticRegression,\n",
    "        #     \"kwargs\": {\n",
    "        #         \"class_weight\": None,    # class_weight: dict or ‘balanced’, default=None\n",
    "        #         \"max_iter\": 1000,        # max_iter: int, default=100\n",
    "        #         \"n_jobs\": -1             # -1 means using all processors\n",
    "        #     }\n",
    "        # }\n",
    "        \"model\": {\n",
    "            \"class\": KNeighborsClassifier,\n",
    "            \"kwargs\": {\n",
    "                \"n_neighbors\": 5,\n",
    "                \"weights\": 'uniform',  # weights: {‘uniform’, ‘distance’} or callable, default=’uniform’\n",
    "                \"algorithm\": 'auto',  # algorithm: {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, default=’auto’\n",
    "                \"p\": 2,  # p: int, default=2 (p = 1 --> l1, manhattan; p = 2 --> l2, euclidean)\n",
    "                \"n_jobs\": -1  # -1 means using all processors\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"yeast\": {\n",
    "        \"name\": \"Yeast Data Set\",\n",
    "        \"url\": \"https://archive.ics.uci.edu/ml/datasets/Yeast\",\n",
    "        \"header\": [0],\n",
    "        \"drop_cols\": [\"sequence name\"],  # columns to drop\n",
    "        \"categorical_vars\": {\n",
    "            \"erl\": {\n",
    "                \"class\": OneHotEncoder,\n",
    "                \"kwargs\": {\"dtype\": int}\n",
    "            },\n",
    "            \"pox\": {\n",
    "                \"class\": OneHotEncoder,\n",
    "                \"kwargs\": {\"dtype\": int}\n",
    "            }\n",
    "        },\n",
    "        \"target\": \"local. site\",  # the label of the dependent variable (i.e., feature)\n",
    "        \"scaler\": {\n",
    "            \"class\": MinMaxScaler,\n",
    "            \"feature_range\": (-1, 1)\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"class\": KNeighborsClassifier,\n",
    "            \"kwargs\": {}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def accuracy_and_auroc(\n",
    "        algo: str, model: BaseEstimator, original_data: np.ndarray, imputed_data: np.ndarray, target: np.ndarray,\n",
    "        verbose: bool = False) -> Tuple[float, float]:\n",
    "    score_accuracy: float = 0\n",
    "    score_auroc: float = 0\n",
    "    scaler: MinMaxScaler = MinMaxScaler(feature_range=((0.00, 1.00) if algo == 'GAIN' else (-1.00, +1.00)))\n",
    "    original: np.ndarray = scaler.fit_transform(X=original_data.copy())\n",
    "    imputed: np.ndarray = scaler.transform(X=imputed_data)\n",
    "    target = target.to_numpy()\n",
    "    model.fit(X=original, y=target)\n",
    "    score_accuracy = cross_val_score(model,X=imputed, y=target, cv=KFold(n_splits=2) )\n",
    "\n",
    "    ''' score_accuracy = accuracy_score(y_true=target, y_pred=model.predict(X=imputed))\n",
    "\n",
    "    if len(np.unique(target)) > 2:  # multiclass case\n",
    "        score_auroc = roc_auc_score(\n",
    "            y_true=target, y_score=model.predict_proba(X=imputed), multi_class='ovr')\n",
    "    else:  # binary case\n",
    "        score_auroc = roc_auc_score(\n",
    "            y_true=target, y_score=model.predict_proba(X=imputed)[:, 1], multi_class='ovr')\n",
    "    if verbose:\n",
    "        print(\"accuracy_and_auroc():\")\n",
    "        print(f\"\\taccuracy score: {score_accuracy:.4f}\")\n",
    "        print(f\"\\tauroc score:    {score_auroc:.4f}\")\n",
    "        '''\n",
    "    return score_accuracy, 0\n",
    "\n",
    "\n",
    "def report(args: Namespace,\n",
    "           model: BaseEstimator,\n",
    "           results: Dict[str, Dict[str, Dict[str, List[Union[np.ndarray, float]]]]]) -> None:\n",
    "    print(f\"miss rate:    {args.miss_rate}\")\n",
    "    print(f\"batch size:   {args.batch_size}\")\n",
    "    print(f\"alpha:        {args.alpha}\")\n",
    "    print(f\"clip values:  ({-1 * args.clip_value}, {+1 * args.clip_value})\")\n",
    "    print(f\"optimizer:    {args.optimizer}\")\n",
    "    print(f\"learn. rate:  {args.learn_rate}\")\n",
    "    if args.optimizer == 'GDA':\n",
    "        pass\n",
    "    elif args.optimizer == 'RMSProp':\n",
    "        print(f\"decay:        {args.decay}\")\n",
    "        print(f\"momentum:     {args.momentum}\")\n",
    "        print(f\"epsilon:      {args.epsilon}\")\n",
    "    else:  # if args.optimizer == 'Adam':\n",
    "        print(f\"beta 1:       {args.beta_1}\")\n",
    "        print(f\"beta 2:       {args.beta_2}\")\n",
    "        print(f\"epsilon:      {args.epsilon}\")\n",
    "    print(f\"# iterations: {args.n_iterations}\")\n",
    "    print(f\"# critic:     {args.n_critic}\")\n",
    "    print(f\"# runs:       {args.n_runs}\")\n",
    "    print(f\"verbose:      {args.verbose}\")\n",
    "    print(f\"model:        {model.__str__()}\")\n",
    "    for dataset, dataset_results in results.items():\n",
    "        print(f\"dataset: {dataset}\")\n",
    "        # print(f\"\\tshape:        {data_shape}\")\n",
    "        for algo, algo_results in dataset_results.items():\n",
    "            print(f\"\\talgorithm: {algo}\")\n",
    "            print(f\"\\t\\trmse:             {np.mean(algo_results['rmse_lst']):.4f} \"\n",
    "                  f\"({np.std(algo_results['rmse_lst']):.4f})\")\n",
    "            print(f\"\\t\\trmse list:        {algo_results['rmse_lst']}\")\n",
    "            print(f\"\\t\\texec. time (s):   {np.mean(algo_results['exec_lst']):.4f} \"\n",
    "                  f\"({np.std(algo_results['exec_lst']):.4f})\")\n",
    "            print(f\"\\t\\texec. times list: {algo_results['exec_lst']}\")\n",
    "            print(f\"\\t\\taccuracy:         {np.mean(algo_results['accuracy_lst']):.4f} \"\n",
    "                  f\"({np.std(algo_results['accuracy_lst']):.4f})\")\n",
    "            print(f\"\\t\\taccuracy list:    {algo_results['accuracy_lst']}\")\n",
    "            print(f\"\\t\\tauroc:            {np.mean(algo_results['auroc_lst']):.4f} \"\n",
    "                  f\"({np.std(algo_results['auroc_lst']):.4f})\")\n",
    "            print(f\"\\t\\tauroc list:       {algo_results['auroc_lst']}\")\n",
    "\n",
    "\n",
    "def main(args: Namespace,unknown) -> None:\n",
    "    algos: List[str] = [algo.strip() for algo in args.algos.split(',')]\n",
    "    algos_set: Set[str] = set(['GAIN', 'SGAIN', 'WSGAIN-CP', 'WSGAIN-GP'])  # TODO: GET RID OF HARDCODED\n",
    "    datasets: List[str] = [dataset.strip() for dataset in args.datasets.split(',')]\n",
    "    datasets_set: Set[str] = set(['breast', 'credit', 'eeg', 'iris', 'letter',  # TODO: GET RID OF HARDCODED\n",
    "                                  'news', 'spam', 'wine-red', 'wine-white', 'yeast',\n",
    "                                  'dataAllYear', 'dataSubset', '2002_BRA'])  # TODO: GET RID OF HARDCODED\n",
    "    callables: Dict[str, Callable[[Namespace, Tuple[int, int], Dict[str, Any]], np.ndarray]] = {\n",
    "        'GAIN': gain, 'SGAIN': SGAIN, 'WSGAIN-CP': WSGAIN_CP, 'WSGAIN-GP': WSGAIN_GP}  # TODO: GET RID OF HARDCODED\n",
    "    results: Dict[str, Dict[str, Dict[str, List[Union[np.ndarray, float]]]]]\n",
    "\n",
    "    if algos == ['ALL']:\n",
    "        algos = sorted(algos_set)\n",
    "    else:\n",
    "        if not set(algos).issubset(algos_set):\n",
    "            raise ValueError(f\"The following algorithms are NOT supported: {set(algos) - algos_set}\")\n",
    "    if datasets == ['ALL']:\n",
    "        datasets = sorted(datasets_set)\n",
    "    else:\n",
    "        if not set(datasets).issubset(datasets_set):\n",
    "            raise ValueError(f\"The following datasets are NOT supported: {set(datasets) - datasets_set}\")\n",
    "\n",
    "    results = {dataset: {algo: {'rmse_lst': [], 'exec_lst': [], 'accuracy_lst': [], 'auroc_lst': []} for algo in algos}\n",
    "               for dataset in datasets}\n",
    "\n",
    "    for run in range(args.n_runs):\n",
    "        tqdm.write(f\"run: {run}\")  # \"helps\" in long runs\n",
    "\n",
    "        data: np.ndarray\n",
    "        miss: np.ndarray\n",
    "        mask: np.ndarray\n",
    "        imputed_data: np.ndarray\n",
    "        model: BaseEstimator\n",
    "        score_accuracy: float\n",
    "        score_auroc: float\n",
    "        t0: float\n",
    "        t1: float\n",
    "        df: pd.DataFrame\n",
    "\n",
    "        for dataset in datasets:\n",
    "            tqdm.write(f\"dataset: {dataset}\")  # \"helps\" in long runs\n",
    "\n",
    "            data, miss, mask = data_loader(data_name=dataset, miss_rate=args.miss_rate)\n",
    "            # data, miss, mask, trgt = matrices_and_target(dataset=args.dataset, miss_rate=args.miss_rate)\n",
    "            df = pd.read_csv(f\"./datasets/{dataset}.csv\")\n",
    "\n",
    "            df[DATASETS[dataset][\"target\"]] = LabelEncoder().fit_transform(df[DATASETS[dataset][\"target\"]])\n",
    "\n",
    "            for algo in algos:\n",
    "                t0 = time()\n",
    "                if algo in ['SGAIN', 'WSGAIN-CP', 'WSGAIN-GP']:\n",
    "                    imputed_data = callables[algo](\n",
    "                        data=miss,\n",
    "                        algo_parameters={key.strip(): value for key, value in args.__dict__.items()}).execute()\n",
    "                else:  # if algo in ['GAIN']:\n",
    "                    imputed_data = callables[algo](\n",
    "                        data_x=miss, gain_parameters={key.strip(): value for key, value in args.__dict__.items()})\n",
    "                t1 = time()\n",
    "                results[dataset][algo]['rmse_lst'].append(\n",
    "                    rmse_loss(ori_data=data, imputed_data=imputed_data, data_m=mask))\n",
    "                results[dataset][algo]['exec_lst'].append(t1 - t0)\n",
    "                model = DATASETS[dataset][\"model\"][\"class\"](**DATASETS[dataset][\"model\"][\"kwargs\"])\n",
    "                score_accuracy, score_auroc = accuracy_and_auroc(\n",
    "                    algo=algo,\n",
    "                    model=model,\n",
    "                    original_data=data, imputed_data=imputed_data, target=df[DATASETS[dataset][\"target\"]],\n",
    "                    verbose=False)\n",
    "                results[dataset][algo]['accuracy_lst'].append(score_accuracy)\n",
    "                results[dataset][algo]['auroc_lst'].append(score_auroc)\n",
    "    df = pd.DataFrame(imputed_data)\n",
    "    df.to_csv(\"datasets/imputed_data1.csv\", index=False)\n",
    "    report(args=args, model=model, results=results)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser: ArgumentParser = ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--algos',\n",
    "        help=\"a csv list of the algorithms to run (e.g., 'GAIN,SGAIN,WSGAIN-CP,WSGAIN-GP')\",\n",
    "        # choices=['GAIN', 'SGAIN', 'WSGAIN-CP', 'WSGAIN-GP'],\n",
    "        default='WSGAIN-GP',\n",
    "        type=str)\n",
    "    parser.add_argument(\n",
    "        '--datasets',\n",
    "        help=\"a csv list of datasets short names\",\n",
    "        # choices=['breast', 'cover-type', 'credit', 'eeg', 'iris', 'letter',\n",
    "        #          'mushroom', 'news', 'spam', 'wine-red', 'wine-white', 'yeast'],\n",
    "        default='2002_BRA',\n",
    "        type=str)\n",
    "    parser.add_argument(\n",
    "        '--miss_rate',\n",
    "        help=\"missing data probability\",\n",
    "        default=0.0,\n",
    "        type=float)\n",
    "    parser.add_argument(\n",
    "        '--batch_size',\n",
    "        help=\"number of samples in mini-batch\",\n",
    "        default=256,\n",
    "        type=int)\n",
    "    parser.add_argument(\n",
    "        '--hint_rate',  # NOTE: the algorithms SGAIN, WSGAIN-CP, and WSGAIN-GP do NOT use this parameter,\n",
    "        help='hint probability',  # it is here just because the GAIN algorithm requires the `hint_rate` parameter\n",
    "        default=0.9,\n",
    "        type=float)\n",
    "    parser.add_argument(\n",
    "        '--alpha',\n",
    "        help=\"hyper-parameter to compute generator's loss\",\n",
    "        default=100,\n",
    "        type=float)\n",
    "    parser.add_argument(\n",
    "        '--lambd',\n",
    "        help=\"hyper-parameter to compute critic's loss\",\n",
    "        default=10,\n",
    "        type=float)\n",
    "    parser.add_argument(\n",
    "        '--clip_value',\n",
    "        help=\"clip (penalty) value\",\n",
    "        default=0.01,\n",
    "        type=float)\n",
    "    parser.add_argument(\n",
    "        '--optimizer',\n",
    "        help=\"solvers' optimizer\",\n",
    "        choices=['Adam', 'GDA', 'RMSProp'],\n",
    "        default='Adam',\n",
    "        type=str)\n",
    "    parser.add_argument(\n",
    "        '--learn_rate',\n",
    "        help=\"optimizer's learning rate\",\n",
    "        default=0.01,\n",
    "        type=float)\n",
    "    parser.add_argument(\n",
    "        '--beta_1',\n",
    "        help=\"Adam optimizer's hyper-parameter (1st moment estimates)\",\n",
    "        default=0.900,\n",
    "        type=float)\n",
    "    parser.add_argument(\n",
    "        '--beta_2',\n",
    "        help=\"Adam optimizer's hyper-parameter (2nd moment estimates)\",\n",
    "        default=0.999,\n",
    "        type=float)\n",
    "    parser.add_argument(\n",
    "        '--decay',\n",
    "        help=\"RMSProp optimizer's hyper-parameter (discounting factor for the history/coming gradient)\",\n",
    "        default=0.900,\n",
    "        type=float)\n",
    "    parser.add_argument(\n",
    "        '--momentum',\n",
    "        help=\"RMSProp optimizer's hyper-parameter (a scalar tensor)\",\n",
    "        default=0.000,\n",
    "        type=float)\n",
    "    parser.add_argument(\n",
    "        '--epsilon',\n",
    "        help=\"Adam hyper-parameter to ensure numerical stability or RMSProp hyper-parameter to avoid zero denominator\",\n",
    "        default=1e-08,\n",
    "        type=float)\n",
    "    parser.add_argument(\n",
    "        '--n_iterations',\n",
    "        help=\"number of training iterations\",\n",
    "        default=10000,\n",
    "        type=int)\n",
    "    parser.add_argument(\n",
    "        '--n_critic',\n",
    "        help=\"number of additional iterations to train the critic\",\n",
    "        default=5,\n",
    "        type=int)\n",
    "    parser.add_argument(\n",
    "        '--n_runs',\n",
    "        help=\"number of runs\",\n",
    "        default=2,\n",
    "        type=int)\n",
    "    parser.add_argument(\n",
    "        '--verbose',\n",
    "        help=\"to control verbosity\",\n",
    "        choices=['False', 'True'],  # `bool` type does NOT work as expected\n",
    "        default='False',  # `bool` type does NOT work as expected\n",
    "        type=str)  # `bool` type does NOT work as expected\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    main(args, unknown = parser.parse_known_args())  # rock 'n roll\n",
    "\n",
    "# python main.py --algos=\"GAIN,SGAIN,WSGAIN-CP,WSGAIN-GP\" --datasets=\"iris,yeast\" --miss_rate=0.2 --optimizer=GDA --learn_rate=0.001 --n_iterations=1000 --n_runs=3\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run: 0\n",
      "dataset: 2002_BRA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 1342/3334 [00:36<00:53, 36.91it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 1094\u001B[0m\n\u001B[0;32m   1087\u001B[0m     parser\u001B[38;5;241m.\u001B[39madd_argument(\n\u001B[0;32m   1088\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m--verbose\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m   1089\u001B[0m         help\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mto control verbosity\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1090\u001B[0m         choices\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFalse\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTrue\u001B[39m\u001B[38;5;124m'\u001B[39m],  \u001B[38;5;66;03m# `bool` type does NOT work as expected\u001B[39;00m\n\u001B[0;32m   1091\u001B[0m         default\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFalse\u001B[39m\u001B[38;5;124m'\u001B[39m,  \u001B[38;5;66;03m# `bool` type does NOT work as expected\u001B[39;00m\n\u001B[0;32m   1092\u001B[0m         \u001B[38;5;28mtype\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mstr\u001B[39m)  \u001B[38;5;66;03m# `bool` type does NOT work as expected\u001B[39;00m\n\u001B[0;32m   1093\u001B[0m     args, unknown \u001B[38;5;241m=\u001B[39m parser\u001B[38;5;241m.\u001B[39mparse_known_args()\n\u001B[1;32m-> 1094\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43munknown\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mparser\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparse_known_args\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# rock 'n roll\u001B[39;00m\n\u001B[0;32m   1096\u001B[0m \u001B[38;5;66;03m# python main.py --algos=\"GAIN,SGAIN,WSGAIN-CP,WSGAIN-GP\" --datasets=\"iris,yeast\" --miss_rate=0.2 --optimizer=GDA --learn_rate=0.001 --n_iterations=1000 --n_runs=3\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[3], line 969\u001B[0m, in \u001B[0;36mmain\u001B[1;34m(args, unknown)\u001B[0m\n\u001B[0;32m    965\u001B[0m t0 \u001B[38;5;241m=\u001B[39m time()\n\u001B[0;32m    966\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m algo \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSGAIN\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mWSGAIN-CP\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mWSGAIN-GP\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n\u001B[0;32m    967\u001B[0m     imputed_data \u001B[38;5;241m=\u001B[39m \u001B[43mcallables\u001B[49m\u001B[43m[\u001B[49m\u001B[43malgo\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    968\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmiss\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m--> 969\u001B[0m \u001B[43m        \u001B[49m\u001B[43malgo_parameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[43mkey\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstrip\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__dict__\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitems\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    970\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# if algo in ['GAIN']:\u001B[39;00m\n\u001B[0;32m    971\u001B[0m     imputed_data \u001B[38;5;241m=\u001B[39m callables[algo](\n\u001B[0;32m    972\u001B[0m         data_x\u001B[38;5;241m=\u001B[39mmiss, gain_parameters\u001B[38;5;241m=\u001B[39m{key\u001B[38;5;241m.\u001B[39mstrip(): value \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m args\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m\u001B[38;5;241m.\u001B[39mitems()})\n",
      "File \u001B[1;32m~\\Documents\\uni\\semester 5\\data science lab\\DataLab\\DataLab\\..\\DataLab\\purify\\imputation\\gain.py:411\u001B[0m, in \u001B[0;36mWSGAIN_GP.execute\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    409\u001B[0m     X_mb: np\u001B[38;5;241m.\u001B[39mndarray \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_miss[indices_mb, :]\n\u001B[0;32m    410\u001B[0m     M_mb: np\u001B[38;5;241m.\u001B[39mndarray \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_mask[indices_mb, :]\n\u001B[1;32m--> 411\u001B[0m     Z_mb: np\u001B[38;5;241m.\u001B[39mndarray \u001B[38;5;241m=\u001B[39m M_mb \u001B[38;5;241m*\u001B[39m X_mb \u001B[38;5;241m+\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m M_mb) \u001B[38;5;241m*\u001B[39m \u001B[43mSGAIN\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msample_z\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_rows\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mm_cols\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mm_dim\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    413\u001B[0m     _, D_loss_curr \u001B[38;5;241m=\u001B[39m sess\u001B[38;5;241m.\u001B[39mrun(\n\u001B[0;32m    414\u001B[0m         fetches\u001B[38;5;241m=\u001B[39m[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mD_solver, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mD_loss],\n\u001B[0;32m    415\u001B[0m         feed_dict\u001B[38;5;241m=\u001B[39m{\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mX: X_mb, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mM: M_mb, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mZ: Z_mb})\n\u001B[0;32m    417\u001B[0m _, G_loss_curr, MSE_loss_curr \u001B[38;5;241m=\u001B[39m sess\u001B[38;5;241m.\u001B[39mrun(\n\u001B[0;32m    418\u001B[0m     fetches\u001B[38;5;241m=\u001B[39m[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mG_solver, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mG_loss, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mMSE_loss],\n\u001B[0;32m    419\u001B[0m     feed_dict\u001B[38;5;241m=\u001B[39m{\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mX: X_mb, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mM: M_mb, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mZ: Z_mb})\n",
      "File \u001B[1;32m~\\Documents\\uni\\semester 5\\data science lab\\DataLab\\DataLab\\..\\DataLab\\purify\\imputation\\gain.py:226\u001B[0m, in \u001B[0;36mSGAIN.sample_z\u001B[1;34m(n_rows, m_cols, feature_range)\u001B[0m\n\u001B[0;32m    224\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[0;32m    225\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msample_z\u001B[39m(n_rows: \u001B[38;5;28mint\u001B[39m, m_cols: \u001B[38;5;28mint\u001B[39m, feature_range: Tuple[\u001B[38;5;28mfloat\u001B[39m, \u001B[38;5;28mfloat\u001B[39m] \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m0.01\u001B[39m, \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m0.01\u001B[39m)) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m np\u001B[38;5;241m.\u001B[39mndarray:\n\u001B[1;32m--> 226\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandom\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muniform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeature_range\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhigh\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeature_range\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mn_rows\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mm_cols\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a986e19120c7a15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T22:15:27.693749Z",
     "start_time": "2024-12-14T22:15:27.688457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6955e412-e6bb-4166-afef-1e0535e674e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T22:15:30.045964Z",
     "start_time": "2024-12-14T22:15:30.039194Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-14 23:15:30.042039: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-14 23:15:30.042317: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-14 23:15:30.042396: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-14 23:15:30.042780: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-14 23:15:30.042803: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-12-14 23:15:30.042900: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-14 23:15:30.042952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6717 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:02:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "sess=tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ec99ad9-fc45-48cb-b0db-7667143619ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebce49f6-63c1-4521-b4d1-13c079a8d999",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6089bde0-a445-4910-8e1c-e80cbd868eae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
